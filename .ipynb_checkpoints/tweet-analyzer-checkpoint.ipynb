{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "from dateutil import *\n",
    "from dateutil.relativedelta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r'\\b-\\b', \" \", text) # removing hyphens\n",
    "    text = re.sub(r'\\'s\\b', '', text) # removing \"'s\"\n",
    "    text = re.sub(r'((JUST IN)|BBCBreaking|SkyNewsBreak|cnnbrk)', \"BREAKING\", text)\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = tknzr.tokenize(text)\n",
    "    tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "    tokens = [word for word in tokens if word.lower() not in ['rt','via','ajenews','ap']]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/Users/mishakobiliansky/Documents/datascience/nlp/twitter-news/archive/Oct_12b' # specify your path here\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "num_files = 6 # max number of files to read (6 = 1 hour worth)\n",
    "num_tweets = 100 # number of tweets sufficient to begin analysis\n",
    "keyword = 'BREAKING' # keyword for breaking news\n",
    "cluster_method = 'AP' # clustering method ('AP' = affinity propagation, 'KM' = KMeans, 'SC' = spectral clustering)\n",
    "damping = 0.75 # affinity propagation damping parameter\n",
    "n_clusters = 25 # number of clusters\n",
    "threshold = 5 # count of unique agencies in a cluster to be considered\n",
    "retweet_threshold = 0.3 # retweet per second ratio threshold\n",
    "max_results = 5 # number of results to send per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get last batch number\n",
    "file_numbers = []\n",
    "for name in glob.glob('tweets.*[0-9].*.csv'):\n",
    "    file_numbers.append(int(name.split(\".\")[1]))\n",
    "last_batch = max(file_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if needed run on a batch other than latest\n",
    "#last_batch = 302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# counting result files in folder\n",
    "sent_result = 0\n",
    "for name in glob.glob('results[0-9].csv'):\n",
    "    sent_result +=1\n",
    "print sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412 tweets in dataframe\n"
     ]
    }
   ],
   "source": [
    "# reading files\n",
    "batch_nums = [int(name.split(\".\")[1]) for name in glob.glob('tweets.*[0-9].*.csv') if int(name.split(\".\")[1]) <= last_batch]\n",
    "last_tweets = []\n",
    "for batchnum in sorted(batch_nums, reverse=True):\n",
    "    currentfile = glob.glob('tweets.'+str(batchnum)+'.*.csv')[0]\n",
    "    last_tweets.append(currentfile)\n",
    "    if len(last_tweets) == 1:\n",
    "        df_tweets = pd.read_csv(currentfile)\n",
    "    else:\n",
    "        df_tweets = pd.concat([df_tweets, pd.read_csv(currentfile)], ignore_index=True)\n",
    "    if len(df_tweets) >= num_tweets or len(last_tweets) >= num_files: break    \n",
    "\n",
    "# making dataframe\n",
    "df_from_each_file = (pd.read_csv(f) for f in last_tweets)\n",
    "df_tweets = pd.concat(df_from_each_file, ignore_index=True)\n",
    "print len(df_tweets), 'tweets in dataframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df = df_tweets.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### time difference between tweet download and tweet created\n",
    "clean_df['time_created'] = clean_df['created_at'].apply(parser.parse, ignoretz=True)\n",
    "clean_df['time_created_local'] = clean_df['time_created'].apply(lambda x: x-relativedelta(hours=+7))\n",
    "clean_df['downloaded_at'] = clean_df['downloaded_at'].apply(parser.parse, ignoretz=True)\n",
    "clean_df['delta'] = clean_df['downloaded_at'] - clean_df['time_created_local']\n",
    "clean_df['delta'] = clean_df['delta'].apply(lambda x: x.total_seconds())\n",
    "clean_df['retweet_ratio'] = clean_df['retweet_count'] / clean_df['delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_id = []\n",
    "# if previous result files exist, make sure sent tweets are in current dataset\n",
    "for name in glob.glob('results[0-9].csv'):\n",
    "    result_df = pd.read_csv(name)\n",
    "    result_id.append(int(result_df['id'][0]))\n",
    "    if int(result_df['id'][0]) not in clean_df['id'].tolist():\n",
    "        # add tweet to dataset\n",
    "        print 'adding tweet'\n",
    "        print result_id, result_df['text'][0]\n",
    "        clean_df = pd.concat([result_df,clean_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412, 11)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing urls from tweet text\n",
    "clean_df['text'] = clean_df['text'].apply(lambda x: re.sub(r'(\\s?https?:\\S+\\s?)', '', x))\n",
    "# convert to unicode\n",
    "clean_df['utext'] = clean_df['text'].apply(lambda x: unicode(x, 'utf-8'))\n",
    "# remove non-ascii characters\n",
    "clean_df['utext'] = clean_df['utext'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', \"\", x))\n",
    "# remove empty rows\n",
    "clean_df = clean_df[clean_df['utext'].map(len) > 0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "clean_df['tokens'] = clean_df['utext'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tagging\n",
    "#from nltk import pos_tag\n",
    "#clean_df['tags'] = clean_df['tokens'].apply(pos_tag)\n",
    "\n",
    "# leave nouns only\n",
    "#clean_df['clean_tags'] = clean_df['tags'].apply(lambda x: [tag[0] for tag in x if tag[1] in ['NN','NNS','NNP','NNPS','CC','CD']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412, 412)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate similarity matrix\n",
    "text = [' '.join(i) for i in clean_df['tokens']] #based on tokens\n",
    "#text = [' '.join(i) for i in clean_df['clean_tags']] #based on tags (nouns)\n",
    "vectors = tfidf.fit_transform(text) # based on current dataset\n",
    "#vectors = all_tfidf.transform(text) # based on all tweet vocabulary\n",
    "cosine_matrix = cosine_similarity(vectors)\n",
    "cosine_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 clusters\n"
     ]
    }
   ],
   "source": [
    "# affinity propagation\n",
    "af = AffinityPropagation(affinity='precomputed', damping=0.75).fit(cosine_matrix)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_\n",
    "clean_df['cluster'] = labels\n",
    "n_clusters = len(cluster_centers_indices)\n",
    "print n_clusters, 'clusters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 clusters\n"
     ]
    }
   ],
   "source": [
    "# k-means\n",
    "km = KMeans(n_clusters = n_clusters).fit(cosine_matrix)\n",
    "cluster_centers = km.cluster_centers_\n",
    "labels = km.labels_\n",
    "clean_df['cluster'] = labels\n",
    "n_clusters = len(cluster_centers)\n",
    "print n_clusters, 'clusters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda_python_2.7/anaconda/lib/python2.7/site-packages/sklearn/manifold/spectral_embedding_.py:215: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n"
     ]
    }
   ],
   "source": [
    "# spectral clustering\n",
    "sc = SpectralClustering(affinity = 'precomputed', n_clusters = n_clusters).fit(cosine_matrix)\n",
    "labels = sc.labels_\n",
    "clean_df['cluster'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choosing cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>id</th>\n",
       "      <th>retweet_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.584346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.498681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.286934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.262771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.093402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         agency  id  retweet_ratio\n",
       "cluster                           \n",
       "52            3   4       0.584346\n",
       "11            3   5       0.498681\n",
       "22            2   2       0.286934\n",
       "55            4   5       0.262771\n",
       "9             6   6       0.093402"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the clusters\n",
    "df_clusters = clean_df.groupby('cluster').agg({'id': len,'agency': pd.Series.nunique, 'retweet_ratio': np.mean})\n",
    "df_clusters.sort_values('retweet_ratio', ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_ratio = clean_df.groupby('cluster').agg({'retweet_ratio': np.mean}).reset_index()\n",
    "#cluster_size = clean_df.groupby('cluster').size().reset_index()\n",
    "top4_clusters = cluster_ratio.sort_values('retweet_ratio', ascending = False)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 RT @BBCBreaking: Thai King Bhumibol Adulyadej \"passed away peacefully,\" palace says\n",
      "\n",
      "52 JUST IN: Sec. Kerry on passing of King Bhumibol: \"The US stands with the people of Thailand at this difficult time.…\n",
      "52 JUST IN: Pres. Obama on the passing of King Bhumibol: \"Today we hold the Thai people in our thoughts and prayers.\"…\n",
      "52 World leaders and royalty mourn the death of Thai King Bhumibol Adulyadej\n",
      "======================================================\n",
      "11 RT @BBCBreaking: He ruled for 70 years - the life of Thai King Bhumibol Adulyadej, who has died\n",
      "11 Thailand's King Bhumibol Adulyadej's life in pictures\n",
      "11 RT @AJENews: Life in pictures: Thai King Bhumibol Adulyadej\n",
      "11 When LIFE photographed Thai King Bhumibol Adulyadej in 1960\n",
      "11 Life in pictures: King Bhumibol Adulyadej of Thailand\n",
      "======================================================\n",
      "22 BREAKING: Thai prime minister says Crown Prince Maha Vajiralongkorn will be the new monarch in accordance with the constitution.\n",
      "22 RT @BBCBreaking: Crown Prince Maha Vajiralongkorn to succeed Thai king, prime minister says\n",
      "======================================================\n",
      "55 Thailand's Royal Palace says King Bhumibol, the world's longest-reigning monarch, dies at age 88.\n",
      "55 BREAKING: Royal Palace says Thailand's King Bhumibol, world's longest-reigning monarch, has died at age 88.\n",
      "55 RT @BBCBreaking: Thailand's King Bhumibol Adulyadej, world's longest-reigning monarch, dies aged 88, palace says\n",
      "55 BREAKING: Royal Palace says Thailand's King Bhumibol, world's longest-reigning monarch, has died at age 88. (AP)\n",
      "55 JUST IN: Thailand's Royal Palace says King Bhumibol, the world's longest-reigning monarch, has died at age 88.\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "for cluster in top4_clusters['cluster']:\n",
    "    for entry in clean_df['text'][clean_df['cluster'] == cluster]:\n",
    "        print cluster, entry\n",
    "    print '======================================================'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>agency</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweet_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>786586055134834688</td>\n",
       "      <td>[American, Honey, review, highway, sell]</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>786577873922625536</td>\n",
       "      <td>[American, Honey, review, highway, sell]</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>6</td>\n",
       "      <td>0.003052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                    tokens  \\\n",
       "326  786586055134834688  [American, Honey, review, highway, sell]   \n",
       "339  786577873922625536  [American, Honey, review, highway, sell]   \n",
       "\n",
       "           agency  retweet_count  retweet_ratio  \n",
       "326  The Guardian              0       0.000000  \n",
       "339  The Guardian              6       0.003052  "
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[clean_df['cluster'] == 8][['id','tokens','agency','retweet_count','retweet_ratio']].sort_values('retweet_ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cluster selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing previous clusters\n",
      "790614093602361344\n",
      "7\n",
      "7\n",
      "790636962289946624\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# remove previously sent clusters, if any\n",
    "result_cluster = []\n",
    "print 'removing previous clusters'\n",
    "for x in result_id:\n",
    "    result_cluster.append(int(clean_df[clean_df['id'] == x]['cluster'][:1]))\n",
    "clean_df = clean_df[~clean_df['cluster'].isin(result_cluster)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max unique agencies in a cluster: 20\n",
      "identified 4 clusters with more than 5 unique agencies\n",
      "no breaking-news clusters identified\n"
     ]
    }
   ],
   "source": [
    "# unique agencies => breaking keyword => retweet ratio\n",
    "chosen_cluster = None\n",
    "big_clusters = []\n",
    "breaking_clusters = []\n",
    "high_retweet_clusters = []\n",
    "# 1. Count distinct agencies in a cluster\n",
    "cluster_users = clean_df.groupby('cluster').agency.nunique().reset_index()\n",
    "max_users_cluster = max(cluster_users['agency'])\n",
    "print 'max unique agencies in a cluster: %d' %max_users_cluster\n",
    "big_clusters = cluster_users['cluster'][cluster_users['agency'] >= threshold].tolist()\n",
    "df_chosen = clean_df[clean_df['cluster'].isin(big_clusters)]\n",
    "if big_clusters:\n",
    "\tprint 'identified %d clusters with more than %d unique agencies' %(len(big_clusters),threshold)\n",
    "\t# 2. At least 1 instances of \"BREAKING\"\n",
    "\tfor cluster in big_clusters:\n",
    "\t\tfor entry in df_chosen[df_chosen['cluster'] == cluster]['tokens']:\n",
    "\t\t\tif keyword in ' '.join(entry):\n",
    "\t\t\t\tif cluster not in breaking_clusters:\n",
    "\t\t\t\t\tbreaking_clusters.append(cluster)\n",
    "\tif breaking_clusters:\n",
    "\t\tprint 'identified %d breaking-news clusters' % len(breaking_clusters)\n",
    "\t\tdf_chosen = df_chosen[df_chosen['cluster'].isin(breaking_clusters)]\n",
    "\t\t# 3. Retweet_ratio is high\n",
    "\t\tfor cluster in breaking_clusters:\n",
    "\t\t\tif np.mean(df_chosen[df_chosen['cluster'] == cluster]\n",
    "\t\t\t\t.sort_values('retweet_ratio', ascending = False)[:3]['retweet_ratio']) >= retweet_threshold:\n",
    "\t\t\t\thigh_retweet_clusters.append(cluster)\n",
    "\t\tif high_retweet_clusters:\n",
    "\t\t\tdf_chosen = df_chosen[df_chosen['cluster'].isin(high_retweet_clusters)]\n",
    "\t\t\tprint 'identified %d clusters with high retweet ratio' % len(high_retweet_clusters)\n",
    "\t\t\taverage_ratios = []\n",
    "\t\t\tfor cluster in high_retweet_clusters:\n",
    "\t\t\t\taverage = np.mean(df_chosen[df_chosen['cluster'] == cluster]\n",
    "\t\t\t\t\t.sort_values('retweet_ratio', ascending = False)[:3]['retweet_ratio'])\n",
    "\t\t\t\taverage_ratios.append(average)\n",
    "\t\t\tprint 'retweet ratios (top 3 tweets):', average_ratios\n",
    "\t\t\t# choose cluster with highest retweet ratio\n",
    "\t\t\tchosen_cluster = int(df_chosen.sort_values('retweet_ratio', ascending = False)[:1]['cluster'])\n",
    "\t\t\tdf_chosen = df_chosen[df_chosen['cluster'] == chosen_cluster]\n",
    "\t\t\t# choosing best tweet\n",
    "\t\t\tdf_chosen = df_chosen.sort_values('retweet_ratio', ascending = False)[:1]\n",
    "\t\t\tresult = df_chosen[['id','text','url','agency','created_at']][df_chosen['retweet_ratio'] == max(df_chosen['retweet_ratio'])]\n",
    "\t\t\tsent_result +=1\n",
    "\t\t\tresult_name = 'results' +str(sent_result)\n",
    "\t\t\tresult.to_csv(result_name +'.csv')\n",
    "\t\t\tfor entry in result['text']:\n",
    "\t\t\t\tprint 'selected tweet: %s' % (entry)\n",
    "\t\telse: print 'no clusters with high enough retweet ratio identified'\n",
    "\telse: print 'no breaking-news clusters identified'\n",
    "else: print 'no large enough clusters identified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unique agencies => retweet ratio\n",
    "chosen_cluster = None\n",
    "big_clusters = []\n",
    "#breaking_clusters = []\n",
    "high_retweet_clusters = []\n",
    "\n",
    "# 1. At least 5 distinct agencies in a cluster\n",
    "threshold = 5\n",
    "cluster_users = clean_df.groupby('cluster').agency.nunique().reset_index()\n",
    "max_users_cluster = max(cluster_users['agency'])\n",
    "print 'max unique agencies in a cluster: %d' %max_users_cluster\n",
    "big_clusters = cluster_users['cluster'][cluster_users['agency'] >= threshold].tolist()\n",
    "df_chosen = clean_df[clean_df['cluster'].isin(big_clusters)]\n",
    "if big_clusters:\n",
    "    print 'identified %d clusters with more than %d unique agencies' %(len(big_clusters),threshold)\n",
    "    # 3. Retweet_ratio is high\n",
    "    retweet_threshold = 0.2\n",
    "    for cluster in big_clusters:\n",
    "        if np.mean(df_chosen[df_chosen['cluster'] == cluster]\n",
    "                .sort_values('retweet_ratio', ascending = False)[:3]['retweet_ratio']) >= retweet_threshold:\n",
    "            high_retweet_clusters.append(cluster)\n",
    "    if high_retweet_clusters:\n",
    "        df_chosen = df_chosen[df_chosen['cluster'].isin(high_retweet_clusters)]\n",
    "        print 'identified %d clusters with high retweet ratio' % len(high_retweet_clusters)\n",
    "        average_ratios = []\n",
    "        for cluster in high_retweet_clusters:\n",
    "            average = np.mean(df_chosen[df_chosen['cluster'] == cluster]\n",
    "                .sort_values('retweet_ratio', ascending = False)[:3]['retweet_ratio'])\n",
    "            average_ratios.append(average)\n",
    "        print 'retweet ratios (top 3 tweets):', average_ratios\n",
    "        # choose cluster with highest retweet ratio\n",
    "        chosen_cluster = int(df_chosen.sort_values('retweet_ratio', ascending = False)[:1]['cluster'])\n",
    "        df_chosen = df_chosen[df_chosen['cluster'] == chosen_cluster]\n",
    "        # choosing best tweet\n",
    "        df_chosen = df_chosen.sort_values('retweet_ratio', ascending = False)[:1]\n",
    "        result = df_chosen[['id','text','url','agency','created_at']][df_chosen['retweet_ratio'] == max(df_chosen['retweet_ratio'])]\n",
    "        sent_result +=1\n",
    "        result_name = 'results' +str(sent_result)\n",
    "        result.to_csv(result_name +'.csv', index = False)\n",
    "        for entry in result['text']:\n",
    "            print 'selected tweet: %s' % (entry)\n",
    "    else: print 'no clusters with high enough retweet ratio identified'\n",
    "else: print 'no large enough clusters identified'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### using mitie: name entity extraction\n",
    "from mitie import *\n",
    "from collections import defaultdict\n",
    "\n",
    "mitie_path = '/Applications/anaconda_python_2.7/anaconda/lib/python2.7/site-packages/mitie'\n",
    "sys.path.append(mitie_path)\n",
    "\n",
    "print \"loading NER model...\"\n",
    "ner = named_entity_extractor(mitie_path +'/MITIE-models/english/ner_model.dat')\n",
    "print \"\\nTags output by this NER model:\", ner.get_possible_ner_tags()\n",
    "\n",
    "def entity_extraction(tokens):\n",
    "    entities = ner.extract_entities(tokens)\n",
    "    tweet_entities = [\" \".join(tokens[i] for i in e[0]) for e in entities]\n",
    "    return tweet_entities\n",
    "\n",
    "# extract entities\n",
    "clean_df['entities'] = clean_df['tokens'].apply(entity_extraction)\n",
    "clean_df['_entities_'] = clean_df['entities'].apply(lambda x: ['_'+entry+'_' for entry in x])\n",
    "clean_df['token_entities'] = clean_df['tokens'] + clean_df['_entities_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_tweets = []\n",
    "df_from_each_file = (pd.read_csv(f) for f in glob.glob('tweets.*[0-9].*.csv'))\n",
    "df_root = pd.concat(df_from_each_file, ignore_index=True)\n",
    "df_from_each_file = (pd.read_csv(f) for f in glob.glob('archive/*/tweets.*[0-9].*.csv'))\n",
    "df_archive = pd.concat(df_from_each_file, ignore_index=True)\n",
    "df_all_tweets = pd.concat([df_root, df_archive], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23699"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing urls from tweet text\n",
    "df_all_tweets['text'] = df_all_tweets['text'].apply(lambda x: re.sub(r'(\\s?https?:\\S+\\s?)', '', x))\n",
    "# convert to unicode\n",
    "df_all_tweets['utext'] = df_all_tweets['text'].apply(lambda x: unicode(x, 'utf-8'))\n",
    "# remove non-ascii characters\n",
    "df_all_tweets['utext'] = df_all_tweets['utext'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', \"\", x))\n",
    "# remove empty rows\n",
    "df_all_tweets = df_all_tweets[df_all_tweets['utext'].map(len) > 0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "df_all_tweets['tokens'] = df_all_tweets['utext'].apply(tokenize)\n",
    "all_text = [' '.join(i) for i in df_all_tweets['tokens']] #based on tokens\n",
    "all_tfidf = tfidf.fit(all_text) #TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('keys.txt') as k:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
